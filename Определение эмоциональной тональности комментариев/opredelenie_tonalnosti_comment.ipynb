{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект по определению эмоциональной тоннальности комментариев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "</ul></li><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span><ul class=\"toc-item\"><li><span><a href=\"#Подключение-необходимых-бибилиотек-и-модулей\" data-toc-modified-id=\"Подключение-необходимых-бибилиотек-и-модулей-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Подключение необходимых бибилиотек и модулей</a></span></li><li><span><a href=\"#Функции-для-более-удобной-работы\" data-toc-modified-id=\"Функции-для-более-удобной-работы-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Функции для более удобной работы</a></span></li><li><span><a href=\"#Токенизация-и-лемматизация\" data-toc-modified-id=\"Токенизация-и-лемматизация-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Токенизация и лемматизация</a></span><ul class=\"toc-item\"><li><span><a href=\"#Вывод:\" data-toc-modified-id=\"Вывод:-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Вывод:</a></span></li></ul></li><li><span><a href=\"#Разделение-на-обучающую-и-тестовую-выборки:\" data-toc-modified-id=\"Разделение-на-обучающую-и-тестовую-выборки:-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Разделение на обучающую и тестовую выборки:</a></span></li><li><span><a href=\"#Устранение-дисбаланса-целевого-признака-на-обучающей-выборке\" data-toc-modified-id=\"Устранение-дисбаланса-целевого-признака-на-обучающей-выборке-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Устранение дисбаланса целевого признака на обучающей выборке</a></span></li><li><span><a href=\"#Вывод:\" data-toc-modified-id=\"Вывод:-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Вывод:</a></span></li></ul></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span><ul class=\"toc-item\"><li><span><a href=\"#Векторизация-данных-и-обучение-модели-логистической-регрессии\" data-toc-modified-id=\"Векторизация-данных-и-обучение-модели-логистической-регрессии-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Векторизация данных и обучение модели логистической регрессии</a></span></li><li><span><a href=\"#Применение-NGRAM-в-процессе-обучения\" data-toc-modified-id=\"Применение-NGRAM-в-процессе-обучения-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Применение NGRAM в процессе обучения</a></span></li><li><span><a href=\"#Использование-pipeline-и-кросс-валидации\" data-toc-modified-id=\"Использование-pipeline-и-кросс-валидации-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Использование pipeline и кросс-валидации</a></span></li><li><span><a href=\"#LinearSVC\" data-toc-modified-id=\"LinearSVC-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>LinearSVC</a></span></li><li><span><a href=\"#RandomForestClassifier\" data-toc-modified-id=\"RandomForestClassifier-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>RandomForestClassifier</a></span></li><li><span><a href=\"#LGBMClassifier\" data-toc-modified-id=\"LGBMClassifier-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>LGBMClassifier</a></span></li><li><span><a href=\"#Вывод:\" data-toc-modified-id=\"Вывод:-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>Вывод:</a></span></li></ul></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка\n",
    "\n",
    "### Подключение необходимых бибилиотек и модулей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re \n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "# Задаем опцию отображения результатов расчетов в ячейках\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функции для более удобной работы\n",
    "\n",
    "Подключим функцию для отображения датафреймов: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_preview(df,number_of_rows=-1):\n",
    "    if number_of_rows==-1:\n",
    "        display(HTML(df.to_html()))\n",
    "    else:    \n",
    "        display(HTML(df.head(number_of_rows).to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка и изучение данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r'/datasets/toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      "text     159571 non-null object\n",
      "toxic    159571 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на содержимое датафрейма:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\n\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember what page that's on?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the tools well.  · talk \"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Your vandalism to the Matt Shirvington article has been reverted.  Please don't do it again, or you will be banned.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to you. Anyway, I'm not intending to write anything in the article(wow they would jump on me for vandalism), I'm merely requesting that it be more encyclopedic so one can use it for school as a reference. I have been to the selective breeding page but it's almost a stub. It points to 'animal breeding' which is a short messy article that gives you no info. There must be someone around with expertise in eugenics? 93.161.107.169</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>alignment on this subject and which are contrary to those of DuLithgow</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_preview(df, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть подозрения, что целевой признак имеет неравномерное распределение. Проверим данные на наличие дисбаланса: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    143346\n",
       "1     16225\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наши подозрения оправдались. Количество положительных ответов меньше числа отрицательных ответов примерно в 10 раз. Что говорит о сильном дисбалансе - после разделения данных на выборки, нам нужно будет провести масштабирование обучающей выборки. \n",
    "\n",
    "### Токенизация и лемматизация\n",
    "\n",
    "Переведем наши тексты в списки состоящие из отдельных слов и приведем их к нормальной форме. Для чего напишем функцию: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(text):\n",
    "    # Преобразуем текст к нижнему регистру\n",
    "    text=text.lower() \n",
    "    # Удалим ненужные символы\n",
    "    text=re.sub(r'[^a-zA-Z ]', ' ', text) \n",
    "    # Уберем лишние пробелы\n",
    "    text=text.split() \n",
    "    text=' '.join(text)\n",
    "\n",
    "    # Токенизируем и лемматизируем текст + сформируем список лемматизированных слов\n",
    "    stemmer = SnowballStemmer(language=\"english\")\n",
    "    tokens = word_tokenize(text)\n",
    "    lemm_list=[]\n",
    "    for token in tokens:\n",
    "        lemm_list.append(stemmer.stem(token))\n",
    "\n",
    "    # Превратим полученный список обратно в текст    \n",
    "    text=''\n",
    "    for word in lemm_list:            \n",
    "        text+=word+' '\n",
    "            \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для наглядного отображения текста добавим в наш датафрейм столбец копию, чтобы потом сравнить \"До\" и \"После\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 55s, sys: 585 ms, total: 3min 55s\n",
      "Wall time: 3min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['oldtext']=df['text']\n",
    "df['text']=df['text'].apply(text_processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[['oldtext','text','toxic']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на данные, которые у нас получились: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>oldtext</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27</td>\n",
       "      <td>explan whi the edit made under my usernam hardcor metallica fan were revert they weren t vandal just closur on some gas after i vote at new york doll fac and pleas don t remov the templat from the talk page sinc i m retir now</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)</td>\n",
       "      <td>d aww he match this background colour i m seem stuck with thank talk januari utc</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.</td>\n",
       "      <td>hey man i m realli not tri to edit war it s just that this guy is constant remov relev inform and talk to me through edit instead of my talk page he seem to care more about the format than the actual info</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\n\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"</td>\n",
       "      <td>more i can t make ani real suggest on improv i wonder if the section statist should be later on or a subsect of type of accid i think the refer may need tidi so that they are all in the exact same format ie date format etc i can do that later on if no one els doe first if you have ani prefer for format style on refer or want to do it yourself pleas let me know there appear to be a backlog on articl for review so i guess there may be a delay until a review turn up it s list in the relev form eg wikipedia good articl nomin transport</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember what page that's on?</td>\n",
       "      <td>you sir are my hero ani chanc you rememb what page that s on</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the tools well.  · talk \"</td>\n",
       "      <td>congratul from me as well use the tool well talk</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>cocksuck befor you piss around on my work</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Your vandalism to the Matt Shirvington article has been reverted.  Please don't do it again, or you will be banned.</td>\n",
       "      <td>your vandal to the matt shirvington articl has been revert pleas don t do it again or you will be ban</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to you. Anyway, I'm not intending to write anything in the article(wow they would jump on me for vandalism), I'm merely requesting that it be more encyclopedic so one can use it for school as a reference. I have been to the selective breeding page but it's almost a stub. It points to 'animal breeding' which is a short messy article that gives you no info. There must be someone around with expertise in eugenics? 93.161.107.169</td>\n",
       "      <td>sorri if the word nonsens was offens to you anyway i m not intend to write anyth in the articl wow they would jump on me for vandal i m mere request that it be more encycloped so one can use it for school as a refer i have been to the select breed page but it s almost a stub it point to anim breed which is a short messi articl that give you no info there must be someon around with expertis in eugen</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>alignment on this subject and which are contrary to those of DuLithgow</td>\n",
       "      <td>align on this subject and which are contrari to those of dulithgow</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_preview(df,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вывод: \n",
    "\n",
    "Как видно из приведенных выше данных, наши тексты разбиты на отдельные слова, переведены к нижнему регистру, очищенны от ненужных и мусорных знаков и символов. Также все слова приведены к нормальной форме(лемматизированы). \n",
    "\n",
    "### Разделение на обучающую и тестовую выборки: \n",
    "\n",
    "Выделим целевой и обучающие признаки: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.drop(['toxic'], axis=1)\n",
    "target = df['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test, target_train, target_test = train_test_split(features\n",
    "                                                                           ,target\n",
    "                                                                           ,test_size=.25\n",
    "                                                                           ,random_state=12345\n",
    "                                                                           ,stratify=target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Устранение дисбаланса целевого признака на обучающей выборке\n",
    "\n",
    "Для устранения дисбаланса опробуем две техники upsempling и downsampling после чего в процессе обучения моделей оценим их эффективность и выберем лучшую для нашей задачи: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(features, target, fraction):\n",
    "    features_zeros = features[target == 0]\n",
    "    target_zeros = target[target == 0]\n",
    "    print('Количество отзывов с нетоксичными описаниями до уменьшения выборки:',len(features_zeros))\n",
    "        \n",
    "    features_ones = features[target == 1]\n",
    "    target_ones = target[target == 1]\n",
    "    print('Количество отзывов с токсичными описаниями до уменьшения выборки:',len(features_ones))\n",
    "\n",
    "    features_downsampled = pd.concat([features_zeros.sample(frac=fraction, random_state=12345)] + [features_ones])\n",
    "    target_downsampled = pd.concat([target_zeros.sample(frac=fraction, random_state=12345)] + [target_ones])\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    features_zeros = features_downsampled[target_downsampled == 0]\n",
    "    print('Количество отзывов с нетоксичными описаниями после уменьшения выборки:',len(features_zeros))          \n",
    "          \n",
    "    features_ones = features_downsampled[target_downsampled == 1]\n",
    "    print('Количество отзывов с токсичными описаниями до уменьшения выборки:',len(features_ones))\n",
    "    \n",
    "    features_downsampled, target_downsampled = shuffle(features_downsampled, target_downsampled, random_state=12345)\n",
    "    \n",
    "    return features_downsampled, target_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(features, target, repeat):\n",
    "    features_zeros = features[target == 0]\n",
    "    target_zeros = target[target == 0]\n",
    "    print('Количество отзывов с нетоксичными описаниями до увеличения выборки:',len(features_zeros))\n",
    "    \n",
    "    features_ones = features[target == 1]\n",
    "    target_ones = target[target == 1]\n",
    "    print('Количество отзывов с токсичными описаниями до увеличения выборки:',len(features_ones))\n",
    "\n",
    "    features_upsampled = pd.concat([features_zeros] + [features_ones] * repeat)\n",
    "    target_upsampled = pd.concat([target_zeros] + [target_ones] * repeat)\n",
    "    \n",
    "    print()\n",
    "          \n",
    "    features_zeros = features_upsampled[target_upsampled == 0]\n",
    "    print('Количество отзывов с нетоксичными описаниями после увеличения выборки:',len(features_zeros))          \n",
    "          \n",
    "    features_ones = features_upsampled[target_upsampled == 1]\n",
    "    print('Количество отзывов с токсичными описаниями после увеличения выборки:',len(features_ones))         \n",
    "    \n",
    "    features_upsampled, target_upsampled = shuffle(features_upsampled, target_upsampled, random_state=12345)\n",
    "    \n",
    "    return features_upsampled, target_upsampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим описанные выше функции к обучающей выборке: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество отзывов с нетоксичными описаниями до уменьшения выборки: 107509\n",
      "Количество отзывов с токсичными описаниями до уменьшения выборки: 12169\n",
      "\n",
      "Количество отзывов с нетоксичными описаниями после уменьшения выборки: 12170\n",
      "Количество отзывов с токсичными описаниями до уменьшения выборки: 12169\n"
     ]
    }
   ],
   "source": [
    "features_train_downsample,target_train_downsample=downsample(features_train,target_train,0.1132)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество отзывов с нетоксичными описаниями до увеличения выборки: 107509\n",
      "Количество отзывов с токсичными описаниями до увеличения выборки: 12169\n",
      "\n",
      "Количество отзывов с нетоксичными описаниями после увеличения выборки: 107509\n",
      "Количество отзывов с токсичными описаниями после увеличения выборки: 109521\n"
     ]
    }
   ],
   "source": [
    "features_train_upsample,target_train_upsample=upsample(features_train,target_train,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод: \n",
    "Мы произвели обработку и подготовили данные к использованию в моделях машинного обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение\n",
    "\n",
    "### Векторизация данных и обучение модели логистической регрессии\n",
    "\n",
    "Для начала создадим итоговый датафрейм, для оценки эффективности моделей ML, куда будем заносить результаты по каждой из них, для последующего сравнения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Result = pd.DataFrame({'Модель':[], 'Метрика F1':[]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведем непосредственно векторизацию, для этого загрузим список английских стоп-слов:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk_stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим качество модели логистической регрессии на выборке сбалансированной техникой downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = features_train_downsample\n",
    "target_train = target_train_downsample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переведем тексты в векторный вид: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.31 s, sys: 24 ms, total: 5.33 s\n",
      "Wall time: 5.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords)\n",
    "tf_idf_model = count_tf_idf.fit_transform(features_train['text'])\n",
    "tf_idf_train = count_tf_idf.transform(features_train['text'])\n",
    "tf_idf_test = count_tf_idf.transform(features_test['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим логистическую регрессию на выборке и оценим метрику качества: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность модели на тестовой выборке: 0.691\n",
      "CPU times: user 8.62 s, sys: 9.28 s, total: 17.9 s\n",
      "Wall time: 17.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=200, class_weight='balanced', random_state=777)\n",
    "model.fit(tf_idf_train, target_train)\n",
    "predict_test = model.predict(tf_idf_test)\n",
    "f1_score_test = f1_score(target_test, predict_test) \n",
    "print('Точность модели на тестовой выборке:', round(f1_score_test,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метрика F1 оказалась не достаточной для прохождения заданного в задаче порога) \n",
    "\n",
    "Запишем результат в итоговую таблицу: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Result=df_Result.append({\n",
    "                            'Модель':'LogisticRegression (downsample)',\n",
    "                            'Метрика F1':round(f1_score_test,3)\n",
    "                           }\n",
    "                           ,ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим метрику на данных сбалансированных с помощью upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = features_train_upsample\n",
    "target_train = target_train_upsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.1 s, sys: 292 ms, total: 26.4 s\n",
      "Wall time: 26.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords)\n",
    "tf_idf_model = count_tf_idf.fit_transform(features_train['text'])\n",
    "tf_idf_train = count_tf_idf.transform(features_train['text'])\n",
    "tf_idf_test = count_tf_idf.transform(features_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность модели на тестовой выборке: 0.764\n",
      "CPU times: user 49.4 s, sys: 41.5 s, total: 1min 30s\n",
      "Wall time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=200, class_weight='balanced', random_state=777)\n",
    "model.fit(tf_idf_train, target_train)\n",
    "predict_test = model.predict(tf_idf_test)\n",
    "f1_score_test = f1_score(target_test, predict_test) \n",
    "print('Точность модели на тестовой выборке:', round(f1_score_test,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно из приведенных выше данных - значение F1-меры значительно увеличилось. Модель преодолевает порог нашей задачи. Попробуем обучить модель на данных сиспользованием биграмм.\n",
    "\n",
    "### Применение NGRAM в процессе обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 8s, sys: 844 ms, total: 1min 9s\n",
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "count_tf_idf = TfidfVectorizer(ngram_range=(1, 2), stop_words=stopwords)\n",
    "tf_idf_model = count_tf_idf.fit_transform(features_train['text'])\n",
    "tf_idf_train = count_tf_idf.transform(features_train['text'])\n",
    "tf_idf_test = count_tf_idf.transform(features_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность модели на тестовой выборке: 0.768\n",
      "CPU times: user 1min 40s, sys: 49.9 s, total: 2min 30s\n",
      "Wall time: 2min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=200, random_state=777)\n",
    "model.fit(tf_idf_train, target_train)\n",
    "predict_test = model.predict(tf_idf_test)\n",
    "f1_score_test = f1_score(target_test, predict_test) \n",
    "print('Точность модели на тестовой выборке:', round(f1_score_test,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При применении NGRAM метрика качества незначительно увеличилась. Запишем полученный результат в итоговую таблицу: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Result=df_Result.append({\n",
    "                            'Модель':'LogisticRegression (upsample + ngram)',\n",
    "                            'Метрика F1':round(f1_score_test,3)\n",
    "                           }\n",
    "                           ,ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Использование pipeline и кросс-валидации\n",
    "\n",
    "Ппоробуем использовать pipeline и кросс- валидацию для повышения значения F1^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность модели на тестовой выборке: 0.753\n",
      "CPU times: user 6min 5s, sys: 1min 43s, total: 7min 48s\n",
      "Wall time: 7min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vect = TfidfVectorizer(ngram_range=(1, 2), stop_words=stopwords)\n",
    "tfidf = TfidfTransformer()\n",
    "clf = LogisticRegression(solver='lbfgs', max_iter=200, random_state=777)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect',vect),\n",
    "    ('tfidf',tfidf),\n",
    "    ('clf',clf)\n",
    "])\n",
    "\n",
    "scores = cross_val_score(pipeline, features_train['text'], target_train, cv=3, scoring='f1_micro')\n",
    "pipeline.fit(features_train['text'],target_train)\n",
    "predict_test = pipeline.predict(features_test['text'])\n",
    "f1_score_test = f1_score(target_test, predict_test) \n",
    "print('Точность модели на тестовой выборке:', round(f1_score_test,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат получился неожиданным, значение метрики уменьшилось. Добавим полученные данные в результирующую таблицу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Result=df_Result.append({\n",
    "                            'Модель':'LogisticRegression (upsample + ngram + кросс-валидация)',\n",
    "                            'Метрика F1':round(f1_score_test,3)\n",
    "                           }\n",
    "                           ,ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем другие модели ML, \n",
    "\n",
    "### LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность модели на тестовой выборке: 0.735\n",
      "CPU times: user 1min 3s, sys: 0 ns, total: 1min 3s\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vect = TfidfVectorizer(stop_words=stopwords)\n",
    "tfidf = TfidfTransformer()\n",
    "clf = LinearSVC(max_iter=200, random_state=777)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect',vect),\n",
    "    ('tfidf',tfidf),\n",
    "    ('clf',clf)\n",
    "])\n",
    "\n",
    "scores = cross_val_score(pipeline, features_train['text'], target_train, cv=3, scoring='f1_micro')\n",
    "pipeline.fit(features_train['text'],target_train)\n",
    "predict_test = pipeline.predict(features_test['text'])\n",
    "f1_score_test = f1_score(target_test, predict_test) \n",
    "print('Точность модели на тестовой выборке:', round(f1_score_test,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат - не фонтан. Запишем  его в таблицу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Result=df_Result.append({\n",
    "                            'Модель':'LinearSVC (upsample + ngram + кросс-валидация)',\n",
    "                            'Метрика F1':round(f1_score_test,3)\n",
    "                           }\n",
    "                           ,ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность модели на тестовой выборке: 0.408\n",
      "CPU times: user 2min 8s, sys: 0 ns, total: 2min 8s\n",
      "Wall time: 2min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vect = TfidfVectorizer(stop_words=stopwords)\n",
    "tfidf = TfidfTransformer()\n",
    "clf = RandomForestClassifier(random_state=777, n_estimators=100, max_depth=20, min_samples_split=7, n_jobs=-1)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect',vect),\n",
    "    ('tfidf',tfidf),\n",
    "    ('clf',clf)\n",
    "])\n",
    "\n",
    "scores = cross_val_score(pipeline, features_train['text'], target_train, cv=3, scoring='f1_micro')\n",
    "pipeline.fit(features_train['text'],target_train)\n",
    "predict_test = pipeline.predict(features_test['text'])\n",
    "f1_score_test = f1_score(target_test, predict_test) \n",
    "print('Точность модели на тестовой выборке:', round(f1_score_test,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Да, модель случайного леса - явно не для этой задачи. Запишем итоговые значения в результирующую таблицу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Result=df_Result.append({\n",
    "                            'Модель':'RandomForestClassifier (upsample + ngram + кросс-валидация)',\n",
    "                            'Метрика F1':round(f1_score_test,3)\n",
    "                           }\n",
    "                           ,ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность модели на тестовой выборке: 0.746\n",
      "CPU times: user 17min 49s, sys: 692 ms, total: 17min 50s\n",
      "Wall time: 18min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vect = TfidfVectorizer(stop_words=stopwords)\n",
    "tfidf = TfidfTransformer()\n",
    "clf = LGBMClassifier(random_state=777, n_jobs=-1)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect',vect),\n",
    "    ('tfidf',tfidf),\n",
    "    ('clf',clf)\n",
    "])\n",
    "\n",
    "scores = cross_val_score(pipeline, features_train['text'], target_train, cv=3, scoring='f1_micro')\n",
    "pipeline.fit(features_train['text'],target_train)\n",
    "predict_test = pipeline.predict(features_test['text'])\n",
    "f1_score_test = f1_score(target_test, predict_test) \n",
    "print('Точность модели на тестовой выборке:', round(f1_score_test,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почти дотянулись до заветной планки. Но \"почти\" это не про машинное обучение. Запишем результат в таблицу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Result=df_Result.append({\n",
    "                            'Модель':'LGBMClassifier (upsample + ngram + кросс-валидация)',\n",
    "                            'Метрика F1':round(f1_score_test,3)\n",
    "                           }\n",
    "                           ,ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод:\n",
    "\n",
    "Мы опробовали различные модели машинного обучения для определения тоанльности текстов. Собрали полученные данные в итоговую таблицу. Пришло время анализа результатов. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы\n",
    "\n",
    "Выведем итоговую таблицу для определния попбедитей в нашем соревномании моделей машинного обучения: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Модель</th>\n",
       "      <th>Метрика F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression (upsample + ngram)</td>\n",
       "      <td>0.768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression (upsample + ngram + кросс-валидация)</td>\n",
       "      <td>0.753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LGBMClassifier (upsample + ngram + кросс-валидация)</td>\n",
       "      <td>0.746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LinearSVC (upsample + ngram + кросс-валидация)</td>\n",
       "      <td>0.735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression (downsample)</td>\n",
       "      <td>0.691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RandomForestClassifier (upsample + ngram + кросс-валидация)</td>\n",
       "      <td>0.408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_preview(df_Result.sort_values(by='Метрика F1',ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно из таблицы - попбедителем в нашем соревновании становится **\"Логистическая регрессия\" с тюнингом в виде использования апсемблинга, TF-IDF и биграмм**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 1600,
    "start_time": "2021-10-04T05:02:23.902Z"
   },
   {
    "duration": 5,
    "start_time": "2021-10-04T05:04:06.204Z"
   },
   {
    "duration": 768,
    "start_time": "2021-10-04T05:04:56.332Z"
   },
   {
    "duration": 32,
    "start_time": "2021-10-04T05:05:07.197Z"
   },
   {
    "duration": 112,
    "start_time": "2021-10-04T05:05:55.421Z"
   },
   {
    "duration": 10,
    "start_time": "2021-10-04T05:07:21.549Z"
   },
   {
    "duration": 6,
    "start_time": "2021-10-04T05:16:40.162Z"
   },
   {
    "duration": 6,
    "start_time": "2021-10-04T05:19:38.451Z"
   },
   {
    "duration": 1617,
    "start_time": "2021-10-04T05:20:02.184Z"
   },
   {
    "duration": 4,
    "start_time": "2021-10-04T05:20:03.804Z"
   },
   {
    "duration": 798,
    "start_time": "2021-10-04T05:20:03.811Z"
   },
   {
    "duration": 31,
    "start_time": "2021-10-04T05:20:04.612Z"
   },
   {
    "duration": 111,
    "start_time": "2021-10-04T05:20:04.645Z"
   },
   {
    "duration": 10,
    "start_time": "2021-10-04T05:20:04.759Z"
   },
   {
    "duration": 19,
    "start_time": "2021-10-04T05:20:04.771Z"
   },
   {
    "duration": 31159,
    "start_time": "2021-10-04T05:20:04.793Z"
   },
   {
    "duration": 232955,
    "start_time": "2021-10-04T05:20:56.859Z"
   },
   {
    "duration": 45,
    "start_time": "2021-10-04T05:28:24.708Z"
   },
   {
    "duration": 8,
    "start_time": "2021-10-04T05:28:58.701Z"
   },
   {
    "duration": 16,
    "start_time": "2021-10-04T05:34:18.631Z"
   },
   {
    "duration": 104,
    "start_time": "2021-10-04T05:34:44.231Z"
   },
   {
    "duration": 8,
    "start_time": "2021-10-04T05:38:10.552Z"
   },
   {
    "duration": 8,
    "start_time": "2021-10-04T05:38:12.192Z"
   },
   {
    "duration": 69,
    "start_time": "2021-10-04T05:43:29.963Z"
   },
   {
    "duration": 175,
    "start_time": "2021-10-04T05:43:45.514Z"
   },
   {
    "duration": 5,
    "start_time": "2021-10-04T05:57:53.801Z"
   },
   {
    "duration": 96,
    "start_time": "2021-10-04T05:58:19.809Z"
   },
   {
    "duration": 236,
    "start_time": "2021-10-04T05:59:09.074Z"
   },
   {
    "duration": 13,
    "start_time": "2021-10-04T06:01:06.057Z"
   },
   {
    "duration": 5075,
    "start_time": "2021-10-04T06:02:09.154Z"
   },
   {
    "duration": 5379,
    "start_time": "2021-10-04T06:02:25.371Z"
   },
   {
    "duration": 93,
    "start_time": "2021-10-04T06:03:33.835Z"
   },
   {
    "duration": 7022,
    "start_time": "2021-10-04T06:03:52.714Z"
   },
   {
    "duration": 19738,
    "start_time": "2021-10-04T06:04:08.607Z"
   },
   {
    "duration": 7,
    "start_time": "2021-10-04T06:06:33.237Z"
   },
   {
    "duration": 4,
    "start_time": "2021-10-04T06:07:18.804Z"
   },
   {
    "duration": 6340,
    "start_time": "2021-10-04T06:07:28.662Z"
   },
   {
    "duration": 26853,
    "start_time": "2021-10-04T06:07:46.773Z"
   },
   {
    "duration": 90854,
    "start_time": "2021-10-04T06:11:35.086Z"
   },
   {
    "duration": 71216,
    "start_time": "2021-10-04T06:17:42.464Z"
   },
   {
    "duration": 154905,
    "start_time": "2021-10-04T06:22:13.683Z"
   },
   {
    "duration": 7,
    "start_time": "2021-10-04T06:24:58.668Z"
   },
   {
    "duration": 452635,
    "start_time": "2021-10-04T06:28:03.908Z"
   },
   {
    "duration": 7,
    "start_time": "2021-10-04T06:36:10.392Z"
   },
   {
    "duration": 64858,
    "start_time": "2021-10-04T06:36:14.472Z"
   },
   {
    "duration": 7,
    "start_time": "2021-10-04T06:43:28.811Z"
   },
   {
    "duration": 132780,
    "start_time": "2021-10-04T06:43:43.611Z"
   },
   {
    "duration": 7,
    "start_time": "2021-10-04T06:45:56.394Z"
   },
   {
    "duration": 995427,
    "start_time": "2021-10-04T06:45:59.198Z"
   },
   {
    "duration": 8,
    "start_time": "2021-10-04T07:02:34.628Z"
   },
   {
    "duration": 100,
    "start_time": "2021-10-04T07:05:17.234Z"
   },
   {
    "duration": 1578,
    "start_time": "2021-10-04T07:06:43.612Z"
   },
   {
    "duration": 4,
    "start_time": "2021-10-04T07:06:45.193Z"
   },
   {
    "duration": 779,
    "start_time": "2021-10-04T07:06:45.200Z"
   },
   {
    "duration": 28,
    "start_time": "2021-10-04T07:06:45.982Z"
   },
   {
    "duration": 113,
    "start_time": "2021-10-04T07:06:46.024Z"
   },
   {
    "duration": 10,
    "start_time": "2021-10-04T07:06:46.140Z"
   },
   {
    "duration": 25,
    "start_time": "2021-10-04T07:06:46.152Z"
   },
   {
    "duration": 237625,
    "start_time": "2021-10-04T07:06:46.181Z"
   },
   {
    "duration": 56,
    "start_time": "2021-10-04T07:10:43.808Z"
   },
   {
    "duration": 9,
    "start_time": "2021-10-04T07:10:43.866Z"
   },
   {
    "duration": 21,
    "start_time": "2021-10-04T07:10:43.877Z"
   },
   {
    "duration": 130,
    "start_time": "2021-10-04T07:10:43.900Z"
   },
   {
    "duration": 8,
    "start_time": "2021-10-04T07:10:44.032Z"
   },
   {
    "duration": 22,
    "start_time": "2021-10-04T07:10:44.042Z"
   },
   {
    "duration": 93,
    "start_time": "2021-10-04T07:10:44.066Z"
   },
   {
    "duration": 166,
    "start_time": "2021-10-04T07:10:44.161Z"
   },
   {
    "duration": 5,
    "start_time": "2021-10-04T07:10:44.329Z"
   },
   {
    "duration": 275,
    "start_time": "2021-10-04T07:10:44.337Z"
   },
   {
    "duration": 13,
    "start_time": "2021-10-04T07:10:44.615Z"
   },
   {
    "duration": 5351,
    "start_time": "2021-10-04T07:10:44.631Z"
   },
   {
    "duration": 17945,
    "start_time": "2021-10-04T07:10:49.985Z"
   },
   {
    "duration": 7,
    "start_time": "2021-10-04T07:11:07.932Z"
   },
   {
    "duration": 19,
    "start_time": "2021-10-04T07:11:07.943Z"
   },
   {
    "duration": 26958,
    "start_time": "2021-10-04T07:11:07.964Z"
   },
   {
    "duration": 90914,
    "start_time": "2021-10-04T07:11:34.925Z"
   },
   {
    "duration": 71202,
    "start_time": "2021-10-04T07:13:05.841Z"
   },
   {
    "duration": 150241,
    "start_time": "2021-10-04T07:14:17.046Z"
   },
   {
    "duration": 6,
    "start_time": "2021-10-04T07:16:47.290Z"
   },
   {
    "duration": 471184,
    "start_time": "2021-10-04T07:16:47.299Z"
   },
   {
    "duration": 7,
    "start_time": "2021-10-04T07:24:38.485Z"
   },
   {
    "duration": 63791,
    "start_time": "2021-10-04T07:24:38.495Z"
   },
   {
    "duration": 7,
    "start_time": "2021-10-04T07:25:42.289Z"
   },
   {
    "duration": 129315,
    "start_time": "2021-10-04T07:25:42.298Z"
   },
   {
    "duration": 7,
    "start_time": "2021-10-04T07:27:51.623Z"
   },
   {
    "duration": 1080636,
    "start_time": "2021-10-04T07:27:51.632Z"
   },
   {
    "duration": 51,
    "start_time": "2021-10-04T07:45:52.271Z"
   },
   {
    "duration": 136,
    "start_time": "2021-10-04T07:45:52.326Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
